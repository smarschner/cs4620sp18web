<!-- <h1><strong>PA 2: Ray1</strong></h1> -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p><strong>Do this project alone or in groups of two, as you prefer. </strong>You can use <a href="https://piazza.com/class/jd0ealxkqom6s8">Piazza</a> to help pair yourselves up.

    <h3>Written Part</h3>
    <h3><strong>Due:  Friday February 9th 2017 (11:59pm)</strong></h3>
    <!-- <hr/> -->
    <h4>Q1.</h4> A group of astronauts has just left on their journey to Mars when ground control informs them that there will be a lunar eclipse back home.  The astronauts are asked to take a picture of this rare event from their location in space and report back.  The situation at the moment they take the picture is as follows:

        <p>Note that we use Earth-centered coordinates that match the ones for the sphere in the mesh assignment.  So 1 Earth radius is the unit of measure.
        <ul>
            <li>The sun is positioned 23,679 units away from the Earth (from the center, if you must ask), exactly above the point at latitude and longitude zero, at (0, 0, 23679).
            <li>The moon is positioned at (0, 1, -10) units away from the center of the earth.  It has radius 0.15.
            <li>The spacecraft is at (15, 0, 20) and its camera is looking in the direction (-3/5, 0, -4/5) (which is the ray through the very center of the image)
        </ul>

        <h5>1). Ray Intersection and Shadow Ray</h5>

        <ol type="a">
            <li>In the astronauts' photo, what (x, y, z) point on the Earth's surface appears exactly at the center of the image? Check your work by showing that the point is both on the ray and on the surface of the Earth (plug the point into the ray and sphere equations). See Sec. 4.4.1 of the textbook.
            <li>Is the point (0, .91, -9.88) on the moon in the shadow of the Earth? You can approximate the direction from the point to the sun as (0, 0, 1).
        </ol>

        <h5>2). Lambertian Reflectance</h5>

            <ol type="a">
                <li>Let's assume for this question that colors only have one component (reflectance) instead of three RGB components. Let the point on the Earth at the center of the image be P (calculated above). If the intensity of the sun is 100,000,000 units and we assume only diffuse reflectance from the Earth, with diffuse reflectance 0.5 at P. What is the light reflected from P toward the camera? Again, you can approximate the light direction as (0, 0, 1).
            </ol>
            <h4>Q2. </h4>
            <h5>Normal Interpolation</h5>
            Suppose we have a regular tetrahedron with edge length 2, centered at the origin. The coordinates of the 4 vertices are A (0,-1,1/&radic;<span style="text-decoration:overline;">2</span>), B (1,0,-1/&radic;<span style="text-decoration:overline;">2</span>), C(-1, 0, -1/&radic;<span style="text-decoration:overline;">2</span>), D(0, 1, 1/&radic;<span style="text-decoration:overline;">2</span>). Let E be a point on face ABD, and the ratio of the distance from E to edge AB, edge AD and edge BD is 1:1:4. If we define vertex normal to be the average of the neighboring triangles' normals, and use linear (barycentric) interpolation (See textbook sec 2.7) of vertex normals for shading, what is the unit outward-pointing normal we use at E?


            <h3>Programming Part</h3>
            <h3><strong>Due:  Friday February 16th 2017 (11:59pm)</strong></h3>

You are required to complete the following 4 features for full credit on this assignment:
  <ul>
    <li> Texture coordinates for <tt>Sphere</tt>s 
    <li> <tt>Microfacet</tt> shader 
    <li> <tt>RepeatTexture</tt> 
    <li> <tt>ClampTexture</tt> 
  </ul>

<h4>Introduction</h4>

<p>Ray tracing is a simple and powerful algorithm for rendering images. Within the accuracy of the scene and shading models and with enough computing time, the images produced by a ray tracer can be physically accurate and can appear indistinguishable from real images. Cornell pioneered research in accurate rendering. See <a href="http://www.graphics.cornell.edu/online/box/compare.html">http://www.graphics.cornell.edu/online/box/compare.html</a> for the famous Cornell box, which exists in Rhodes Hall.

<p>The basic idea behind ray tracing is to model the movement of photons as they travel along their path from a light source, to a surface, and finally to a viewer's eye. Tracing forward along this path is difficult, since it can be unclear which photons will make it to the eye, especially when photons can bounce between objects several times along their path (and in unpredictable ways). To simplify the system, these paths are computed in reverse; that is, rays originate from the position of the eye. Color information, which depends on the scene's light sources, is computed when these rays intersect an object, and is sent back towards the eye to determine the color of any particular pixel.

<p>In this assignment, you will complete an implementation of a ray tracer.  It will not be able to produce physically accurate images, but later in the semester you will extend it to produce nice looking images with many interesting effects.

<p>We have provided framework code to save you from spending time on class
hierarchy design and input/output code, and rather to have you focus on implementing the core ray
tracing components. However, you also have the freedom to redesign the system
as long as your ray tracer meets our requirements and produces the same images
for the given scenes.

<h4>Requirement Overview</h4>

<p>The programming portion of this assignment is described below.

<p>A ray tracer computes images in <i>image order</i>, meaning that the outer loop visits pixels one at a time.  (In our framework the outer loop is found in the class <tt>RayTracer</tt>).  For each pixel it performs the following operations:
<ul style="list-style: none;">
  <li><strong>Ray generation</strong>, in which the viewing ray corresponding to the current pixel is found.  The calculations depend on the characteristics of the camera: where it is located, which way it is looking, etc.  In our framework, ray generation happens in the subclasses of <tt>Camera</tt>.
  <li><strong> Ray intersection</strong>, which determines the visible surface at the current pixel by finding the closest intersection between the ray and a surface in the scene (in our framework, ray intersection is handled by the <tt>Scene</tt> class).
  <li><strong>Shading</strong>, in which the color of the object, as seen from the camera, is determined.  Shading accounts for the effects of illumination and shadows (in our framework, shading happens in the subclasses of <tt>Shader</tt>).
</ul>

In this assignment, your ray tracer will have support for:
<ul>
  <li>Spheres and triangle meshes
  <li>Lambertian and Microfacet shading
  <li>Point lights with shadows
  <li>Parallel and perspective cameras
</ul>

<p>Your ray tracer will read files in an XML file format and output PNG images. We have split ray tracing in CS4620 into two parts. For now, you will implement basic ray tracing. Later in the semester, after you have learned many more graphics techniques, you will implement other features, such as advanced shaders, faster rendering, and transformations on groups of objects. In this document, we focus on what you need to implement the basic elements of a ray tracer.

<h5>Precisions in the programming assignment</h5>
<p>All the input in this project is parsed in single precision and the scene is stored in single precision as well.  This is a good practice since scene data in real systems can occupy a lot of memory. But accurate ray intersections and shadows are difficult to achieve with single precision.  So you should carry out all the geometric calculations, and store all related intermediate results, in double precision.

<h4>Getting Started</h4>

<p>A new commit has been pushed to the class Github page in the master branch. We recommend
switching to your master branch, pulling this commit, and creating a new branch (e.g. A2
solution) and committing your work there. This new commit contains all the framework changes and additions necessary for this project.</p>

<p>
	The ray1 framework is set up to output high-dynamic range (HDR) images in the OpenEXR format.  We use the <tt>openexr</tt> C++ library to produce HDR output, via a JNI library called <tt>openexrjni</tt>. In order for the Java runtime to find the native library that is appropriate to your platform, you need to configure your native library location. To do this in Eclipse, go to Project → Properties → Java Build Path → Select Libraries → Select the openexrjni-3.0.0.jar dropdown menu → Modify Native Library Location. Modify this setting so that it matches your OS. Also in Project -> Properties -> Java Build Path → Select Source → Select the a2/src. Drop Down Menu → Modify Native Library Location. Modify this setting so that it matches your OS. 
</p>
<p>
  The HDR output file will have a <tt>.exr</tt> extension. HDR images are becoming fairly widely supported; handy tools you can use to view them include:
  <ul>
  <li><a href="https://bitbucket.org/wkjarosz/hdrview">HDRView</a>, a minimal but convenient viewer that is cross-platform but comes precompiled only for MacOS.
  <li><a href="http://www.irfanview.com/">IrfanView</a>, a mature and full featured image manipulation program that's free for commercial use but is Windows-only.
  <li>The qt4image app, part of <a href="https://bitbucket.org/edgarv/hdritools">HDRITools</a> package from which our JNI library also comes, which is cross-platform but very minimal.
  <li>The Preview app that comes with MacOS, which is handy (you can adjust exposure in the Adjust Color dialog) but has a habit of overwriting your files without asking.
  <li>Photoshop, the widely used image manipulation program, if you already happen to own a license.
  </ul>

  The <tt>writeHDR</tt> variable in <tt>RayTracer</tt> can be set to false to output PNG images, but pixel values above 1.0 will be clipped.
</p>

<h4>Specification and Implementation</h4>

<p>We have marked all the functions or parts of the functions you need to complete with <tt>TODO#A2</tt>
in the source code. To see all these TODO's in Eclipse, select <tt>Search</tt> menu, then <tt>File Search</tt>
and type <tt>"TODO#A2"</tt>. There are quite a few separate code snippets to
complete in this assignment. To check your progress as you go along, we
recommend implementation in the following order.


<h5><strong>Cameras</h5></strong>

<p>An instance of the <tt>Camera</tt> class represents a camera in the scene. It must implement the following methods: 

<pre>
  public void init()
</pre>

This method sets an orthonormal basis for the camera based on the view and up directions. 

<pre>
  public void getRay(Ray outRay, double inU, double inV)
</pre>

This method generates a new ray starting at the camera through a given image point and stores it in <tt>outRay</tt>. The image point is given in UV coordinates, which must first be converted into view coordinates. Recall that an orthographic camera produces rays with varying origin and constant direction, whereas a perspective camera produces rays with constant origin and varying direction.

<p>You will implement these two methods for both the <tt>OrthographicCamera</tt> and the <tt>PerspectiveCamera</tt>. All of the properties you will need are stored in the <tt>Camera</tt> class.

<p>You can then test your getRay() method for both of these classes by running the main method in Camera.java. You should get "Test successful" messages if your implementation is correct.


<h5><strong>Surfaces</h5></strong>

<p>An instance of the <tt>Surface</tt> class represents a piece of geometry in the scene. It has to implement the following method:
<pre>
  boolean intersect(IntersectionRecord outRecord, Ray ray)
</pre>

  <p>  This method intersects the given ray with the surface. Return true if the ray intersects the geometry 
  and write relevant information to the IntersectionRecord structure. Relevant information 
  includes the position of the hit point, the surface normal at the hit point, 
  the value <tt>t</tt> of the ray at the hit point, 
  and the surface being hit itself. See the <tt>IntersectionRecord</tt> class for details.


<p>We ask that you implement the intersection method for the following surfaces:
<ul style="list-style: none;">
  <li><strong><tt>Sphere</strong></tt>: The class contains a 3D point <tt>center</tt> and a float <tt>radius</tt>. You will need to solve the sphere/ray intersection equations. Be sure to calculate the exact normal at the point of intersection and store it in the <tt>IntersectionRecord</tt>. Additionally, please compute and store UV-coordinates in the same orientation as Assignment 1 (i.e. the poles should lie on the y-axis, and the seam should intersect the negative z-axis if the sphere is located at the origin). See section 4.4.1 of the book.
  <li><strong><tt>Triangle</strong></tt>: In our system, each triangle belongs to a <tt>Mesh</tt>, which keeps a reference to a <tt>mesh</tt> object. A triangle stores an <tt>OBJFace</tt> object which corresponds to the face formed by the triangle. See section 4.4.2 of the book.

If the owning face does not have a per-vertex normal, the <tt>intersect</tt>
method should return the triangle normal stored in <tt>Triangle</tt>. If it does,
it should interpolate the normals of the three vertices using the barycentric
coordinate of the hit point. Texture coordinates, if available, should be interpolated using barycentric coordinates as well.
</ul>

<h5><strong>Scene</h5></strong>

<p>The <tt>Scene</tt> class stores information relating to the composition, 
i.e. the camera, a list of surfaces, a list of lights, etc. It also contains a scene-file-specified <tt>exposure</tt> field, which can be used to adjust the brightness of the resulting image. You need to implement the following method: 
<pre>
  boolean intersect(IntersectionRecord outRecord, Ray rayIn, boolean anyIntersection)
</pre>

<p>  The method should loop through all surfaces in the scene (stored in the <tt>Surfaces</tt> field), looking for intersections along <tt>rayIn</tt>.
  This is done by calling the <tt>intersect</tt> method on each surface and it is used when a ray is first cast out into the scene. If <tt>anyIntersection</tt> is false, 
  it should only record the first intersection that happens along <tt>rayIn</tt>, i.e. the intersection   
  with the lowest value of <tt>t</tt>. If there is such an intersection, it is recorded in <tt>outRecord</tt>
  and true is returned; otherwise, the method returns false, and <tt>outRecord</tt> is not modified. 
  If <tt>anyIntersection</tt> is true, the method may return true immediately upon finding any intersection 
  with the scene's geometry, even without setting <tt>outRecord</tt> 
  (this version is used to make shadow calculations faster).



<h5><strong>Ray Tracing</h5></strong>

<p>You then need to complete the <tt>shadeRay</tt> method of the <tt>RayTracer</tt> class. This method contains the main logic of the ray tracer. It takes a ray generated by the camera, intersects it with the scene, and returns a color based on what the ray hits. Fortunately, most of the details are implemented in other methods; all you need to do is call those methods, and this should only take a few lines. For instance, the <tt>Scene</tt> class contains a method for finding the first object a given ray hits. Details are in the code's comments.


<p>We have provided a normal shader that determines color based on the normal of the surface, as well as some sample scenes that use this shader. Once you have completed the previous sections, you should be able to render the scenes <tt>one-sphere-rgb-normals.xml</tt> and <tt>two-boxes-rgb-normals.xml</tt> correctly. See Appendix A for details on rendering scenes.



<h5><strong>Shaders</h5></strong>

<p>A shader is represented by an instance of the <tt>Shader</tt> class. You need to implement the following method:
<pre>
  public void shade(Colord outIntensity, Scene scene, Ray ray, IntersectionRecord record)
</pre> 

  <p>  which sets <tt>outIntensity</tt> (the resulting color) to the fraction of 
the light that comes from the incoming direction \(\omega_i\) (i.e. the direction towards the light) and scatters out in direction \(\omega_o\) (i.e., towards the viewer).



<p>We ask you to implement two shaders:
<ul style="list-style: none;">
  <li><tt><strong>Lambertian</tt></strong>: This class implements the perfectly diffuse shader. Its diffuse reflectance \(R_d\) is specified by  the <tt>diffuseColor</tt> field; this is the fraction of light reflected for each color channel, so the valid range is from 0 to 1, and [0.18, 0.18, 0.18] is a photographer's middle gray. The <tt>shade</tt> function should set 
<tt>outIntensity</tt> to:
  <ul style="list-style: none; text-align: center;">
    \[\frac{I_L}{r^2} \; \frac{R_d}{\pi} \max ( n \cdot \omega_i , 0 ) \]
  </ul>
where n is the normal at the intersection point, \(I_L\) is the
intensity of the light source, and \(r\) is the distance to the light source.
See section 4.5.1, though notice the book's definition does not include the
physically accurate \(1/r^2\) falloff, and uses a &ldquo;diffuse coefficient&rdquo; \(k_d\), which is our \(R_d/\pi\).</p>

<p>If the surface is back-facing to the camera (that is, \(\omega_o \cdot n < 0\)), the camera is seeing the wrong side of the surface, and you should just return black.</p>

<p>Notice that these values should be computed once for each light in the
scene, and the contributions from each light should be summed, unless something
is blocking the path from the light source to the object. <tt>Shader.java</tt>
contains a useful <tt>isShadowed()</tt> method to check this. Overall, the code
for each <tt>shade</tt> method should look something like:

<pre>
 <strong>reset</strong> <tt>outIntensity</tt> to (0, 0, 0)
   <strong>for</strong> each light in the scene
     <strong>if</strong> <tt>!isShadowed(...)</tt>
       compute color contribution from this light
       add this contribution to <tt>outIntensity</tt>
     <strong>end if</strong> 
   <strong>end for</strong> 
</pre>

<p>After the <tt>Lambertian</tt> shader completed, you should be able to render <tt>two-boxes.xml</tt> correctly.

<li><tt><strong>Microfacet</strong></tt>: This class implements a microfacet-based lighting model that includes diffuse reflection and also models light that reflects specularly from the top surface of the material.  This model is defined in terms of its bidirectional reflectance distribution function (BRDF) and <tt>outIntensity</tt> is computed as follows:
  <ul style="list-style: none; text-align: center;">
    \[\frac{I_L}{r^2} \;\left(\frac{R_d}{\pi} + R_m\;f_r(\omega_i, \omega_o, n)\right) \max ( n \cdot \omega_i , 0 ) \]
  </ul>
and just as with the <tt>Lambertian</tt> shader, these computations should be repeated for each visible light in the scene and the contributions summed. </p>

  The term in parentheses is the full BRDF of the surface, including the Lambertian BRDF (which is a constant, since diffuse reflection sends light equally in all directions) and the specular BRDF \(f_r\). Its specular reflectance \(R_m\) is specified by the <tt>microfacetColor</tt> field, representing the fraction of light reflected by specular component in each color channel. And the value of function \(f_r(\omega_i, \omega_o, n)\) here is a number (scalar-valued).

  The specific microfacet model we use comes from <a href="https://www.graphics.cornell.edu/~bjw/microfacetbsdf.pdf">this paper</a> and is product of several factors:
  <ul style="list-style: none; text-align: center;">
    \[ f_r(\omega_i,\omega_o,n) \; =  \; \frac{F(\omega_i,h)G(\omega_i,\omega_o,h)D(h)}{4|\omega_i \cdot n| |\omega_o \cdot n|} \]
  </ul>

  <p><tt>\(\omega_i\)</tt>: Direction unit vector along incoming ray.</p>
  <p><tt>\(\omega_o\)</tt>: Direction unit vector along outgoing ray.</p>
  <p><tt>\(n\)</tt>: unit normal vector of surface at the shaded point.</p>
  <p><tt>\(h\)</tt>: normalized half vector of \(\omega_i\) and \(\omega_o\):
  <ul style="list-style: none; text-align: center;">
    \[  h(\omega_i, \omega_o) \; = \; \frac{\omega_i + \omega_o}{|\omega_i + \omega_o|} \]
  </ul>
  </p>

  <p>The three terms \(F\), \(D\), and \(G\) are the Fresnel factor, the slope distribution, and the shadowing/masking factor, and their particulars are given below.


  <p>There is a BRDF object attribute for microfacet shader class. Its <tt>EvalBRDF</tt> method should be called for BRDF function value evaluation. </p>

  <p>The microfacet model you're required to implement uses the Beckmann distribution to model the roughness of the surface (see below for info about a second model for extra credit).   You are supposed to implement <tt>EvalBRDF</tt> method and all other necessary helper methods in <tt>Beckmann.java</tt>. Parsed microfacet model related parameters can be found as attributes in its super class file <tt>BRDF.java</tt>.</p>

  <p>More details on each factor of the microfacet BRDF function.</p>
  
  <p><tt>\(F(\omega_i,h)\)</tt>: This is the Fresnel term for unpolarized light, and only reflection (not refraction) is considered in this assignment.
  <ul style="list-style: none; text-align: center;">
    \[ F(\omega_i, h) \; = \; \frac{1}{2}\frac{(g-c)^2}{(g+c)^2}\left(1 \; + \; \frac{(c(g+c)-1)^2}{(c(g-c)+1)^2}\right)\]
  </ul>
  where \(g \; = \; \sqrt{\frac{n_t^2}{n_i^2}-1+c^2}\) and \(c \; = \; |\omega_i \cdot h|\). \(n_t\) and \(n_i\) are the refractive index of material and air respectively. \(n_i\;=\;1.0\). In microfacet shader, there is a variable <tt>nt</tt> parsed from xml file and it corresponds to \(n_t\). Note that when computing the value of \(g\), it is possible to encounter a neagive number under the square root in some edge cases.  In this case set \(F\;=\;1\).
</p>

  <p><tt>\(D(h)\)</tt>: This factor comes from the microfacet distribution function. As for the required part of this assignment, we use Beckmann distribution.
  <ul style="list-style: none; text-align: center;">
    \[ D(h) \; = \; \frac{\chi^{+}(h \cdot n)}{\pi\alpha_b^2\cos^4\theta_h} \; \exp\left(\frac{-\tan^2\theta_h}{\alpha_b^2}\right) \]
  </ul>
  where \(\theta_h\) denotes the angle between \(h\) and \(n\); \(\chi^{+}(a)\) is positive characteristic function (one if \(a>0\) and zero if \(a \leq 0\) ). \(alpha_b\) is width parameter of the Beckmann distribution, corresponding to parsed variable <tt>alpha</tt> in <tt>BRDF.java</tt>.</p>

<p>
  <tt>\(G(\omega_i,\omega_o,h)\)</tt>: This is Smith shadowing-masking term; it is designed based on the choice of microfacet distribution function \(D(h)\). For 
  the Beckmann model, we use a rational approximation calculated by Walter et al.  \(G(\omega_i, \omega_o,h)\) can be separated as the product of two parts:
  <ul style="list-style: none; text-align: center;">
    \[ G(\omega_i, \omega_o, h) = G_1(\omega_i,h)G_1(\omega_o,h) \]
  </ul>
  For Beckmann distribution,
  <ul style="list-style: none; text-align: center;">
    \[ 
    G_1(v,h) \; = \; \chi^{+}\left(\frac{v\cdot h}{v\cdot n}\right)
    \begin{cases}
    \frac{3.535a\;+\;2.181a^2}{1\;+\;2.276a\;+\;2.577a^2}, & \text{if}\ a < 1.6 \\
    1 & \text{otherwise}
    \end{cases}
     \]
  </ul>
  where \(a=(\alpha_b\tan\theta_v)^{-1}\) and \(\theta_v\) is the angle between \(v\) and \(n\).
</p>

<p>After both <tt>Lambertian</tt> and <tt>microfacet</tt> (Beckmann) shader are completed, you should be able to render <tt>one-sphere.xml</tt>, <tt>one-sphere2.xml</tt>, <tt>teapot.xml</tt>, <tt>teapot2.xml</tt> (different level of roughness with <tt>teapot.xml</tt>), <tt>wire-box.xml</tt>, <tt>wire-box-orthographic.xml</tt> and the bunny scenes correctly. </p>

<p>To help you ensure that each term in microfacet shader with Beckmann distribution is implemented correctly, please refer to the directory <tt>a2/data/scenes/ray1/microfacet_Beckmann_debug_reference</tt>. For example, if the <tt>shade</tt> method in <tt>Microfacet.java</tt> is correctly implemented and the <tt>EvalBRDF</tt> method in <tt>Beckmann.java</tt> returns just the Fresnel term <tt>F</tt>, rendering <tt>microfacet_Beckmann_debug_reference/one-sphere-debug.xml</tt> should lead to an image the same as <tt>microfacet_Beckmann_debug_reference/one-sphere-F.xml.exr</tt>. The other images in this subdirectory give similar references for the other terms. Using these tests can help you isolate errors in the implementation.</p>
</ul>


<h5><strong>Textures</strong></h5>

<p>The <tt>Shader</tt> classes above have a <tt>texture</tt> field that is by default set to <tt>null</tt>. If the scene XML file specifies an image texture for the shader, a <tt>Texture</tt> object will be initialized in this field. If the texture has been loaded, you should use it to determine the diffuse color in the <tt>shade</tt> method rather than the <tt>diffuseColor</tt> field.

<p>The <tt>Texture</tt> class and its subclasses are responsible for using the UV-coordinates of a particular point on the <tt>Surface</tt> to find this color on a given image (located in the <tt>Texture</tt> class). You will be implementing the <tt>getTexColor(Vector2d texCoord)</tt> method for two very similar subclasses of
 <tt>Texture</tt>. Both classes convert UV-coordinates into the nearest pixel coordinates corresponding to the image. The difference is in the way they handle UV-coordinates outside of the [0.0, 1.0] range:

<ul style="list-style: none;">
  <li><strong><tt>ClampTexture</tt></strong>: This class uses the nearest pixel for UV-coordinates that are out of range. As a result, the colors of the boundary pixels of a texture are extended outwards in UV space.
  <li><strong><tt>RepeatTexture</tt></strong>: This class repeats the texture for out-of-range UV-coordinates. In this case, copies of the texture are tiled across UV space.
</ul>

<p>The details of how to implement these are provided in the code.  Once these are completed, all scenes in subdirectory <tt>textured_scenes</tt> should render correctly except <tt>earth-GGX.xml</tt> and <tt>teapot_textured-GGX.xml</tt>, <tt>teapot_textured2-GGX.xml</tt>.


<h4>Appendix A: Testing</h4>

<p>The <tt>data</tt> directory contains scene files and reference images for the purpose of testing your code. Right click on the 
<tt> RayTracer</tt> file in <tt>Package Explorer</tt>, and choose <tt>Run As->Java Application</tt>. 

Without any command line arguments, the ray tracer will attempt to render all scenes in the default scene directory (i.e. <tt>data/scenes/ray1</tt>).
If you want to render specific scenes, you can list the file names individually on the command line. This can be done from the <tt>Run</tt> dialog (<tt>Run Configurations</tt>). Choose the <tt>Arguments</tt> tab, and provide your command line arguments in <tt>Program arguments</tt>. Additionally, passing in a directory name will render all scenes in that directory.


<p>Note that <tt>RayTracer</tt> prepends 
<tt>data/scenes/ray1</tt> to its arguments. This means that you only need to provide a scene file name to render it. For example, if you give the argument <tt>one-sphere.xml</tt> to the <tt>RayTracer</tt> it will load the scene file <tt>data/scenes/ray1/one-sphere.xml</tt> and produce an output image in the same directory as the scene file.

<p>If you would like to change the default scene location, there is a <tt>-p</tt> command line option that allows you to specify a different default path instead of using <tt>data/scenes/ray1</tt>. 


<p>Compare the images you have with the reference images, if there are visually
significant differences, consider checking for bugs in your code.

<h4>Appendix B: Extra Credit</h4>
<tt><strong>Cylinder surface</strong></tt>
<ul>
  <p>We've included a <tt>Cylinder</tt> subclass of <tt>Surface</tt>, similar to the <tt>Triangle</tt> and <tt>Sphere</tt> classes. The class contains <tt>center</tt>, a 3D point, and <tt>radius</tt>, <tt>height</tt>, both real numbers. Assume that the cylinder is capped, i.e., the top and bottom have caps (it is not a hollow tube). Also, assume it is aligned with the z-axis, and is centered around <tt>center</tt>; <br>
  i.e., if
  <tt>center</tt> is \((c_x, c_y, c_z)\) and <tt>height</tt> is \(h\), the cylinder's z-extent is from \(c_z-\frac{h}{2}\) to \(c_z+\frac{h}{2}\).

  <p>Your task is to implement the <tt>intersect</tt> method of a mathematical cylinder (NOT a meshed cylinder). Add an XML scene that includes a cylinder to test your implementation (See Appendix C for details on how this should be done--you shouldn't need to change any parser code).

  <p>Once this is done, try implementing a cylinder that is arbitrarily oriented. You will need to add two parameters, <tt>rotX</tt> and <tt>rotY</tt>, to specify the cylinder's rotation about the x and y axes. Note that this will require changing the coordinate system of the incoming ray within the <tt> intersect</tt> method to account for these rotations. After the coordinate system change, the math used to intersect the ray with the cylinder should be the same as its axis-aligned variant. Finally, build a test scene with a cylinder object.
</ul>
<tt><strong>Microfacet shader with GGX distribution</strong></tt>
<ul>
  <p>The microfacet-based shader implemented above assumes the normal distribution of the microfacet surface is Beckmann. Another class of normal distribution that is widely used is GGX. In order to implement microfacet shader with GGX distribution, both the Smith shadowing-masking factor <tt>\(G(\omega_i,\omega_o,h)\)</tt> and the microfacet distribution factor <tt>\(D(h)\)</tt> should be modified. 
  The GGX distribution with width parameter <tt>\(\alpha_g\)</tt> is 
  <ul style="list-style: none; text-align: center;">
    \[ D(h) \; = \; \frac{\alpha_g^2\;\chi^{+}(h \cdot n)}{\pi\cos^4\theta_h\;(\alpha_g^2\;+\;\tan^2\theta_h)^2} \]
  </ul>
  The corresponding shadowing-masking term is
  <ul style="list-style: none; text-align: center;">
    \[ G(\omega_i, \omega_o, h) = G_1(\omega_i,h)G_1(\omega_o,h) \]
  </ul>
  And for GGX distribution,
  <ul style="list-style: none; text-align: center;">
    \[ 
    G_1(v,h) \; = \; \chi^{+}\left(\frac{v\cdot h}{v\cdot n}\right)
    \; \frac{2}{1\;+\;\sqrt{1\;+\;\alpha_g^2\tan^2\theta_v}}
     \]
  </ul>
  The meaning of the variable notations and all other factors in the BRDF function keep the same as above.</p>

  <p>To get extra credit for <tt>GGX</tt> shader, <tt>EvalBRDF</tt> and other necessary helper methods in <tt>GGX.java</tt> should be implemented. The width parameter <tt>\(alpha_g\)</tt> is parsed as variable <tt>alpha</tt> from super class <tt>BRDF.java</tt>.</p>
  <p>After the microfacet shader with GGX distribution correctly implemented, <tt>one-sphere-GGX.xml</tt>, <tt>one-sphere2-GGX.xml</tt>, <tt>teapot-GGX.xml</tt>, <tt>teapot2-GGX.xml</tt>, <tt>textured_scenes/earth-GGX.xml</tt> and <tt>textured_scenes/teapot_textured-GGX.xml</tt> can be rendered.</p>
</ul>

<h4>Appendix C: Framework</h4>

<p>This section describes the framework code that comes with the assignment. You do not need to spend time trying to understand it and it is not essential to read this to get started on the assignment. Instead, you can reference it as needed.

<p>The framework for this assignment includes a simple main program,
some utility classes for vector math, a parser for the input file
format, and stubs for the classes that are required by the parser.

<h5><strong>RayTracer</strong></h5>

<p>This class holds the entry point for the program. The <tt>main</tt>
method is provided, so that your program will have a command-line
interface compatible with ours. It treats each command line argument
as the name of an input file, which it parses, renders an image, and
writes the image to a PNG file. The method <tt>RayTracer.renderImage</tt> is called to do the actual rendering.

<h5><strong>Image</strong></h5>

<p>This class contains an array of pixel colors (stored as doubles) and the requisite
code to get and set pixels and to output the image to a PNG file.

<h5><strong><tt>egl.math</tt> package</strong></h5>

<p>This package contains classes to represent 2D and 3D vectors,
as well as RGB colors. They support all the standard vector arithmetic
operations you're likely to need, including dot and cross products
for vectors and addition and scaling for colors.

<h5><strong>Parser</strong></h5>

<p>The <tt>Parser</tt> class contains a simple parser based on Java's built-in XML
parsing. The parser simply reads a XML document and instantiates an object for
each XML entity, adding it to its containing element by calling
<tt>set...</tt> or <tt>add...</tt> methods on the containing object.

<p>For instance, the input:
<pre>
&lt;scene&gt;
    &lt;surface type="Sphere"&gt;
        &lt;shader type="Lambertian"&gt;
            &lt;diffuseColor&gt;0 0 1&lt;/diffuseColor&gt;
        &lt;/shader&gt;
        &lt;center&gt;1 2 3&lt;/center&gt;
        &lt;radius&gt;4&lt;/radius&gt;
    &lt;/surface&gt;
&lt;/scene&gt;
</pre>
results in the following construction sequence:
<ul>
  <li>Create the scene.
  <li>Create an object of class <tt>Sphere</tt> and add it to the scene
by calling <tt>Scene.addSurface</tt>. This is OK because <tt>Sphere</tt>
extends the <tt>Surface</tt> class.
  <li>Create an object of class <tt>Lambertian</tt> and add it to the sphere using <tt>Sphere.setShader</tt>. 
This is OK because <tt>Lambertian</tt> extends the <tt>Shader</tt> class.
 <li>Call <tt>setDiffuseColor(new Colord(0, 0, 1))</tt> on the shader.
 <li>Call <tt>setCenter(new Vector3d(1, 2, 3))</tt> on the sphere.
 <li>Call <tt>setRadius(4)</tt> on the sphere.
</ul>

<p>Which elements are allowed where in the file is determined by which
classes contain appropriate methods, and the types of those methods'
parameters determine how the tag's contents are parsed (as a number,
a vector, etc.). There is more detail for the curious in the header
comment of the <tt>Parser</tt> class.

<p>The practical result of all this is that your ray tracer is handed
an object of class <tt>Scene</tt> that contains objects that are in one-to-one correspondence
with the elements in the input file. You shouldn't need to change
the parser in any way.

<h4>What to Submit</h4>
<p>Submit the written part solution as a PDF file with Q1 and Q2 together.
<p>As for the code part, submit a zip file containing your solution organized the same way as the code on Github, along with any additional files.
Include a readme in your zip that contains:
<ul>
  <li>You and your partner's names and NetIDs.
  <li>Any problems with your solution.
  <li>Anything else you want us to know.
</ul>
<p><strong><a href=https://cms.csuglab.cornell.edu>Upload here (CMS)</a></strong>
